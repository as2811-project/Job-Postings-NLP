{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Anandh Sellamuthu\n",
    "#### Student ID: S3976934\n",
    "\n",
    "Date: 14th Sept, 2023\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* re\n",
    "* nltk\n",
    "* os\n",
    "* numpy (as np)\n",
    "* pandas (as pd)\n",
    "* nltk.probability\n",
    "* nltk.RegexpTokenizer\n",
    "* nltk.tokenize.sent_tokenize\n",
    "* itertools.chain\n",
    "* nltk.tokenize.word_tokenize\n",
    "\n",
    "## Introduction\n",
    "In this task, we'll be pre-processing the given files. Pre-processing includes tasks such as tokenizing, removing single character tokens, removing tokens based on various statistics (top 50 words in terms of occurrence, for instance). This task covers only the descriptions present in each job posting file. Post-completion, the necessary output file is exported along with tokenized descriptions which will be used for tasks 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_webindex_title_and_description(file_content):\n",
    "    webindex = None\n",
    "    title = None\n",
    "    description = None\n",
    "\n",
    "    lines = file_content.split('\\n')  # Decode the bytes to a string\n",
    "    in_description = False\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Webindex:\"):\n",
    "            webindex = line.split(\":\")[1].strip()\n",
    "            in_description = False\n",
    "        elif line.startswith(\"Title:\"):\n",
    "            title = line.split(\":\")[1].strip()\n",
    "        elif line.startswith(\"Description:\") or in_description:\n",
    "            if description is None:\n",
    "                description = line.lstrip(\"Description:\").strip()\n",
    "                in_description = True\n",
    "            else:\n",
    "                description += \" \" + line.strip()\n",
    "    \n",
    "    return webindex, title, description\n",
    "\n",
    "# Load data\n",
    "jp_data = load_files(r\"data\", encoding=\"utf-8\")\n",
    "job_ads = jp_data.data  # List of job advertisements\n",
    "\n",
    "#Sorting filenames in order to ensure the order in which the files are read and the tokens are appended to the lists will remain the same.\n",
    "sorted_filenames = sorted(jp_data.filenames, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "# Initialize lists to store webindices, titles, and descriptions\n",
    "webindices = []\n",
    "titles = []\n",
    "descriptions = []\n",
    "\n",
    "# Extract webindices, titles, and descriptions for each job ad\n",
    "for filename in sorted_filenames:\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        webindex, title, description = extract_webindex_title_and_description(file.read())\n",
    "        webindices.append(webindex)\n",
    "        titles.append(title)\n",
    "        descriptions.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_data['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/Healthcare_Nursing/Job_00547.txt', 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetching a random filename and its corresponding target/category for verification\n",
    "jp_data['filenames'][2], jp_data['target'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are loaded and the information that we'll be using for this task are stored in-memory within lists. This includes the Web Index values present in each file along with the titles and descriptions. There are four categories of job postings, namely, Accounting_Finance, Engineering, Healthcare_Nursing, and Sales. The files are read in a sorted order (based on file names). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data\n",
    "In this section, all the pre-processing steps mentioned in the introduction is performed. Each pre-processing step is within its own sub-section for ease of reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all descriptions to lowercase\n",
    "descriptions = [description.lower() for description in descriptions]\n",
    "\n",
    "#Regex\n",
    "pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "\n",
    "# Tokenize the descriptions using the custom pattern\n",
    "tokenized_descriptions = [re.findall(pattern, description) for description in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_print(tokenized_descriptions):\n",
    "    words = list(chain.from_iterable(tokenized_descriptions)) # we put all the tokens in the corpus in a single list\n",
    "    vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of descriptions:\", len(tokenized_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw jobs:\n",
      " our client, a profitable estate agent is looking to recruit telesales negotiators with excellent communication skills to join their team. a prime candidate will possess previous sales negotiating and customer service experience with an estate agency. candidate must be able to adopt a flexible working attitude to working hours as evening and weekend working will be required. full training is given with induction. in return you will earn a competitive basic salary with a fantastic bonus structure. \n",
      "\n",
      "Tokenized descriptions:\n",
      " ['our', 'client', 'a', 'profitable', 'estate', 'agent', 'is', 'looking', 'to', 'recruit', 'telesales', 'negotiators', 'with', 'excellent', 'communication', 'skills', 'to', 'join', 'their', 'team', 'a', 'prime', 'candidate', 'will', 'possess', 'previous', 'sales', 'negotiating', 'and', 'customer', 'service', 'experience', 'with', 'an', 'estate', 'agency', 'candidate', 'must', 'be', 'able', 'to', 'adopt', 'a', 'flexible', 'working', 'attitude', 'to', 'working', 'hours', 'as', 'evening', 'and', 'weekend', 'working', 'will', 'be', 'required', 'full', 'training', 'is', 'given', 'with', 'induction', 'in', 'return', 'you', 'will', 'earn', 'a', 'competitive', 'basic', 'salary', 'with', 'a', 'fantastic', 'bonus', 'structure']\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw jobs:\\n\",descriptions[0],'\\n')\n",
    "print(\"Tokenized descriptions:\\n\",tokenized_descriptions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9834\n",
      "Total number of tokens:  186952\n",
      "Lexical diversity:  0.052601737344345076\n",
      "Total number of descriptions: 776\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenized_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few sub-tasks focus on removing words based on statistics. This involves removing single character tokens, stop words and top 50 words that appear most often. The idea behind this process is that these tokens/words don't add much information. It could potentially only add noise and cause performance degredation when it comes to training classifiers (which will be done in Tasks 2 and 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Removing single character tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_descriptions = [[d for d in descriptions if len(d) >=2] for descriptions in tokenized_descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the tokens with lengths greater than or equal to 2 are retained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['client', 'profitable', 'estate', 'agent', 'recruit', 'telesales', 'negotiators', 'excellent', 'communication', 'skills', 'join', 'team', 'prime', 'candidate', 'possess', 'previous', 'sales', 'negotiating', 'customer', 'service', 'experience', 'estate', 'agency', 'candidate', 'adopt', 'flexible', 'working', 'attitude', 'working', 'hours', 'evening', 'weekend', 'working', 'required', 'full', 'training', 'induction', 'return', 'earn', 'competitive', 'basic', 'salary', 'fantastic', 'bonus', 'structure']\n"
     ]
    }
   ],
   "source": [
    "#Reading the given stopwords file into a set\n",
    "with open('stopwords_en.txt', 'r') as stopwords_file:\n",
    "    custom_stopwords = set(stopwords_file.read().split())\n",
    "\n",
    "#Function to return words not in the set created above\n",
    "def remove_custom_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in custom_stopwords]\n",
    "\n",
    "tokenized_descriptions = [remove_custom_stopwords(tokens) for tokens in tokenized_descriptions]\n",
    "print(tokenized_descriptions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9404\n",
      "Total number of tokens:  107161\n",
      "Lexical diversity:  0.0877558066834016\n",
      "Total number of descriptions: 776\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenized_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Removing words that appear only once using term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(tokenized_descriptions)) # we put all the tokens in the corpus in a single list\n",
    "vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each word in tokenized_descriptions\n",
    "word_counts = Counter(word for tokens in tokenized_descriptions for word in tokens)\n",
    "\n",
    "# Get the least common words (sorted by frequency in ascending order)\n",
    "least_common_words = word_counts.most_common()[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4186"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the frequency of each word in the list of words\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Find words that appear only once\n",
    "unique_words = {word for word, count in word_counts.items() if count == 1}\n",
    "\n",
    "# Remove words that appear only once from the list of words\n",
    "filtered_words = [word for word in words if word not in unique_words]\n",
    "\n",
    "# If needed, recreate the vocabulary set from the filtered words\n",
    "filtered_vocab = set(filtered_words)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_frequency_words(tokenized_descriptions):\n",
    "    # Flatten the list of tokens\n",
    "    all_tokens = [token for doc_tokens in tokenized_descriptions for token in doc_tokens]\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Find single-frequency words\n",
    "    single_frequency_words = set(word for word, count in word_counts.items() if count == 1)\n",
    "    \n",
    "    # Remove single-frequency words from each document\n",
    "    cleaned_jobs = [[token for token in doc_tokens if token not in single_frequency_words] for doc_tokens in tokenized_descriptions]\n",
    "    \n",
    "    return cleaned_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing single frequency words via the function defined above\n",
    "tokenized_descriptions = remove_single_frequency_words(tokenized_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5218\n",
      "Total number of tokens:  102975\n",
      "Lexical diversity:  0.05067249332362224\n",
      "Total number of descriptions: 776\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenized_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Removing top 50 most frequent words based on Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable([set(job) for job in tokenized_descriptions]))\n",
    "doc_fd = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating document frequency for each word\n",
    "document_frequency = {word: doc_fd.freq(word) for word in doc_fd.keys()}\n",
    "\n",
    "#Sorting to retrieve top 50 words by frequency\n",
    "top_50_words_by_df = sorted(document_frequency.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "\n",
    "#Extracting said words from the top 50 list\n",
    "top_50_words = [word for word, _ in top_50_words_by_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove top N number of words (by Document Frequency)\n",
    "def remove_top_words(tokens, top_words):\n",
    "    return [word for word in tokens if word not in top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_descriptions = [remove_top_words(tokens, top_50_words) for tokens in tokenized_descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5168\n",
      "Total number of tokens:  81205\n",
      "Lexical diversity:  0.06364140139153993\n",
      "Total number of descriptions: 776\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenized_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started off with a vocabulary size of 9834. With each pre-processing step, a more compact version of the vocabulary is returned. Post pre-processing, the vocabulary size is 5168. With that, we can go ahead and export the file in the required naming convention/format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file 'vocab.txt' has been created.\n"
     ]
    }
   ],
   "source": [
    "#Creating the sorted vocabulary list\n",
    "sorted_vocab = sorted(list(set(word for tokens in tokenized_descriptions for word in tokens)))\n",
    "\n",
    "#Creating a dictionary to map words to integer indices\n",
    "word_to_index = {word: index for index, word in enumerate(sorted_vocab)}\n",
    "\n",
    "#Creating the vocab.txt file\n",
    "with open('vocab.txt', 'w', encoding='utf-8') as vocab_file:\n",
    "    #Writing each word and its corresponding index to the file\n",
    "    for word, index in word_to_index.items():\n",
    "        vocab_file.write(f\"{word}:{index}\\n\")\n",
    "\n",
    "print(\"Vocabulary file 'vocab.txt' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_jobs(Filename,tokenized_descriptions):\n",
    "    out_file = open(Filename, 'w') #Opens a file to write the descriptions\n",
    "    string = \"\\n\".join([\" \".join(jobs) for jobs in tokenized_descriptions])\n",
    "    out_file.write(string)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_jobs('tokenized_descriptions.txt',tokenized_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this task, all the pre-processing steps specified in the specifications for Task 1 were performed. The required output file, vocab.txt, was generated along with the tokenised descriptions which will be used in tasks 2 and 3. This task helped in gaining insights on the dataset provided and paved the way for the upcoming tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Swast, T 2009, ‘Answer to “Unicode (UTF-8) reading and writing to files in Python”’, Stack Overflow.\n",
    "\n",
    "‘Unicode (UTF-8) reading and writing to files in Python’, <https://www.w3docs.com/snippets/python/unicode-utf-8-reading-and-writing-to-files-in-python.html>.\n",
    "\n",
    "Acknowledgement: Some of the codes used here were repurposed from the lab and activity jupyter notebooks provided on Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
